{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "916bcb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModel, AutoModel, TFBertForSequenceClassification, TFAutoModelForSequenceClassification, AutoModelForSequenceClassification\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import tensorflow as tf\n",
    "from models import *\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import transformers\n",
    "import shap\n",
    "import matplotlib.pylab as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "import torch.nn.functional as F\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from happytransformer import HappyTextToText, TTSettings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb23db3",
   "metadata": {},
   "source": [
    "# Wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b12d222",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Hubert\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Hubert\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08aeb82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/16/2022 12:26:31 - INFO - happytransformer.happy_transformer -   Using model: cpu\n"
     ]
    }
   ],
   "source": [
    "happy_tt = HappyTextToText(\"T5\", \"vennify/t5-base-grammar-correction\")\n",
    "args = TTSettings(num_beams=5, min_length=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dd6961b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synonyms(word):\n",
    "    synonyms = []\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lm in syn.lemmas():\n",
    "                 synonyms.append(lm.name().replace('_', ' '))#adding into synonyms\n",
    "    return list(set(synonyms))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1418fc",
   "metadata": {},
   "source": [
    "# HateSpeach model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c0b7a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_tokenizer = AutoTokenizer.from_pretrained(\"unitary/toxic-bert\")\n",
    "\n",
    "hate_model = AutoModelForSequenceClassification.from_pretrained(\"unitary/toxic-bert\")\n",
    "\n",
    "pred = transformers.pipeline(\"text-classification\", model=hate_model, tokenizer=hate_tokenizer, return_all_scores=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081f182c",
   "metadata": {},
   "source": [
    "# SentenceSimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b053cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "ss_tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "ss_model = AutoModel.from_pretrained('sentence-transformers/all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c95e8e3",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "292d19ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_hate(sentence, hate_model=hate_model, hate_tokenizer=hate_tokenizer):\n",
    "    y = hate_model(**hate_tokenizer(sentence, return_tensors='pt'))\n",
    "    return scipy.special.expit(y.logits.detach().numpy())[0]\n",
    "\n",
    "def eval_ss(input_sentence1, input_sentence2, ss_model=ss_model, ss_tokenizer=ss_tokenizer):\n",
    "    \n",
    "    input_sentences = [input_sentence1, input_sentence2]\n",
    "    \n",
    "    encoded_input = ss_tokenizer(input_sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = ss_model(**encoded_input)\n",
    "\n",
    "    # Perform pooling. In this case, max pooling.\n",
    "    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "    return torch.sqrt(torch.sum((sentence_embeddings[0] - sentence_embeddings[1])**2)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55dd1e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sugestion(input_sentence, word_idx,\n",
    "                   labels=[\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"],\n",
    "                  args=args):\n",
    "    \n",
    "    sentence = input_sentence\n",
    "    split_sentence = sentence.split(' ')\n",
    "    bad_word = split_sentence[word_idx]\n",
    "    bad_word = bad_word.replace('!', '').replace('?', '').replace('.', '').replace(',', '')\n",
    "    print(f'original sentence: {input_sentence}')\n",
    "    original_hate = eval_hate(sentence)\n",
    "    for i in range(len(labels)):\n",
    "        print(f'{labels[i]} score: {100*(round(original_hate[i], 2))}%')\n",
    "        \n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    ss_tab = []\n",
    "    hate_tab = []\n",
    "    synoms = get_synonyms(bad_word)\n",
    "    synoms.append('')\n",
    "    result_tab = []\n",
    "    love_tab = []\n",
    "    \n",
    "    for synom in synoms:\n",
    "        new_sentence = split_sentence.copy()\n",
    "        new_sentence[word_idx] = synom\n",
    "        new_sentence = ' '.join(new_sentence)\n",
    "        \n",
    "        result = happy_tt.generate_text(f\"grammar: {new_sentence}\", args=args).text\n",
    "        hate = eval_hate(result) - eval_hate(sentence)\n",
    "        result_tab.append(result)\n",
    "        hate_tab.append(hate)\n",
    "        ss_tab.append(eval_ss(sentence, result))\n",
    "        love_tab.append(np.sum(hate))\n",
    "    \n",
    "    \n",
    "    hate_tab = np.array(hate_tab)\n",
    "    ss_tab = np.array(ss_tab)\n",
    "    result_tab = np.array(result_tab)\n",
    "    sort_arr = np.argsort(love_tab)\n",
    "    \n",
    "    hate_tab = hate_tab[sort_arr]\n",
    "    ss_tab = ss_tab[sort_arr]\n",
    "    result_tab = result_tab[sort_arr]\n",
    "    \n",
    "    for k in range(hate_tab.shape[0]):\n",
    "        print(result_tab[k])\n",
    "        for i in range(hate_tab.shape[1]):\n",
    "            print(labels[i], \": \", 100*(np.round(hate_tab[k, i] / original_hate[i], 2)), \"%\")\n",
    "    \n",
    "        print(f'Similaryty score: {round(ss_tab[k], 2)}')\n",
    "        print()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b577211",
   "metadata": {},
   "source": [
    "# Ewaluacja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "824f78d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original sentence: I will hurt you\n",
      "toxic score: 83.99999737739563%\n",
      "severe_toxic score: 7.999999821186066%\n",
      "obscene score: 5.000000074505806%\n",
      "threat score: 75.99999904632568%\n",
      "insult score: 7.000000029802322%\n",
      "identity_hate score: 1.9999999552965164%\n",
      "\n",
      "\n",
      "I will smart you.\n",
      "toxic :  -100.0 %\n",
      "severe_toxic :  -100.0 %\n",
      "obscene :  -100.0 %\n",
      "threat :  -100.0 %\n",
      "insult :  -100.0 %\n",
      "identity_hate :  -99.00000095367432 %\n",
      "Similaryty score: 2.8299999237060547\n",
      "\n",
      "I will distress you.\n",
      "toxic :  -99.00000095367432 %\n",
      "severe_toxic :  -100.0 %\n",
      "obscene :  -99.00000095367432 %\n",
      "threat :  -100.0 %\n",
      "insult :  -99.00000095367432 %\n",
      "identity_hate :  -98.00000190734863 %\n",
      "Similaryty score: 2.5999999046325684\n",
      "\n",
      "I will anguish you.\n",
      "toxic :  -98.00000190734863 %\n",
      "severe_toxic :  -100.0 %\n",
      "obscene :  -99.00000095367432 %\n",
      "threat :  -100.0 %\n",
      "insult :  -99.00000095367432 %\n",
      "identity_hate :  -99.00000095367432 %\n",
      "Similaryty score: 2.5399999618530273\n",
      "\n",
      "I will you.\n",
      "toxic :  -98.00000190734863 %\n",
      "severe_toxic :  -100.0 %\n",
      "obscene :  -98.00000190734863 %\n",
      "threat :  -100.0 %\n",
      "insult :  -99.00000095367432 %\n",
      "identity_hate :  -98.00000190734863 %\n",
      "Similaryty score: 3.0899999141693115\n",
      "\n",
      "I will detriment you.\n",
      "toxic :  -93.99999976158142 %\n",
      "severe_toxic :  -100.0 %\n",
      "obscene :  -98.00000190734863 %\n",
      "threat :  -100.0 %\n",
      "insult :  -99.00000095367432 %\n",
      "identity_hate :  -95.99999785423279 %\n",
      "Similaryty score: 2.180000066757202\n",
      "\n",
      "I will offend you.\n",
      "toxic :  -85.00000238418579 %\n",
      "severe_toxic :  -99.00000095367432 %\n",
      "obscene :  -95.99999785423279 %\n",
      "threat :  -99.00000095367432 %\n",
      "insult :  -98.00000190734863 %\n",
      "identity_hate :  -94.9999988079071 %\n",
      "Similaryty score: 2.4700000286102295\n",
      "\n",
      "I will ache you.\n",
      "toxic :  -66.00000262260437 %\n",
      "severe_toxic :  -99.00000095367432 %\n",
      "obscene :  -92.00000166893005 %\n",
      "threat :  -98.00000190734863 %\n",
      "insult :  -94.9999988079071 %\n",
      "identity_hate :  -92.00000166893005 %\n",
      "Similaryty score: 2.2100000381469727\n",
      "\n",
      "I will weaken you.\n",
      "toxic :  -60.00000238418579 %\n",
      "severe_toxic :  -95.99999785423279 %\n",
      "obscene :  -93.00000071525574 %\n",
      "threat :  -82.99999833106995 %\n",
      "insult :  -92.00000166893005 %\n",
      "identity_hate :  -79.00000214576721 %\n",
      "Similaryty score: 2.369999885559082\n",
      "\n",
      "I will spite you.\n",
      "toxic :  -37.00000047683716 %\n",
      "severe_toxic :  -99.00000095367432 %\n",
      "obscene :  -87.00000047683716 %\n",
      "threat :  -98.00000190734863 %\n",
      "insult :  -85.00000238418579 %\n",
      "identity_hate :  -91.00000262260437 %\n",
      "Similaryty score: 2.4600000381469727\n",
      "\n",
      "I will pain you.\n",
      "toxic :  -15.999999642372131 %\n",
      "severe_toxic :  -77.99999713897705 %\n",
      "obscene :  -62.99999952316284 %\n",
      "threat :  -44.999998807907104 %\n",
      "insult :  -73.00000190734863 %\n",
      "identity_hate :  -66.00000262260437 %\n",
      "Similaryty score: 1.5\n",
      "\n",
      "I will traumatize you.\n",
      "toxic :  -20.999999344348907 %\n",
      "severe_toxic :  -74.00000095367432 %\n",
      "obscene :  -72.00000286102295 %\n",
      "threat :  -28.00000011920929 %\n",
      "insult :  -66.00000262260437 %\n",
      "identity_hate :  -50.0 %\n",
      "Similaryty score: 2.3299999237060547\n",
      "\n",
      "I will scathe you.\n",
      "toxic :  1.9999999552965164 %\n",
      "severe_toxic :  -76.99999809265137 %\n",
      "obscene :  -55.000001192092896 %\n",
      "threat :  -50.999999046325684 %\n",
      "insult :  -30.000001192092896 %\n",
      "identity_hate :  -54.00000214576721 %\n",
      "Similaryty score: 2.430000066757202\n",
      "\n",
      "I will bruise you.\n",
      "toxic :  -18.000000715255737 %\n",
      "severe_toxic :  -70.99999785423279 %\n",
      "obscene :  -58.99999737739563 %\n",
      "threat :  -23.000000417232513 %\n",
      "insult :  -67.00000166893005 %\n",
      "identity_hate :  -52.99999713897705 %\n",
      "Similaryty score: 2.2899999618530273\n",
      "\n",
      "I will injury you.\n",
      "toxic :  -18.99999976158142 %\n",
      "severe_toxic :  -64.99999761581421 %\n",
      "obscene :  -62.99999952316284 %\n",
      "threat :  -18.99999976158142 %\n",
      "insult :  -61.000001430511475 %\n",
      "identity_hate :  -41.999998688697815 %\n",
      "Similaryty score: 1.9500000476837158\n",
      "\n",
      "I will wound you.\n",
      "toxic :  -18.000000715255737 %\n",
      "severe_toxic :  -66.00000262260437 %\n",
      "obscene :  -57.999998331069946 %\n",
      "threat :  -15.999999642372131 %\n",
      "insult :  -61.000001430511475 %\n",
      "identity_hate :  -40.99999964237213 %\n",
      "Similaryty score: 2.3299999237060547\n",
      "\n",
      "I will damage you.\n",
      "toxic :  -10.999999940395355 %\n",
      "severe_toxic :  -61.000001430511475 %\n",
      "obscene :  -50.0 %\n",
      "threat :  -20.999999344348907 %\n",
      "insult :  -56.99999928474426 %\n",
      "identity_hate :  -49.000000953674316 %\n",
      "Similaryty score: 1.7999999523162842\n",
      "\n",
      "I will injure you.\n",
      "toxic :  -15.000000596046448 %\n",
      "severe_toxic :  -61.000001430511475 %\n",
      "obscene :  -62.99999952316284 %\n",
      "threat :  -14.000000059604645 %\n",
      "insult :  -56.00000023841858 %\n",
      "identity_hate :  -15.000000596046448 %\n",
      "Similaryty score: 1.9299999475479126\n",
      "\n",
      "I will harm you.\n",
      "toxic :  -9.000000357627869 %\n",
      "severe_toxic :  -55.000001192092896 %\n",
      "obscene :  -50.0 %\n",
      "threat :  -11.999999731779099 %\n",
      "insult :  -51.99999809265137 %\n",
      "identity_hate :  -21.99999988079071 %\n",
      "Similaryty score: 1.5700000524520874\n",
      "\n",
      "I will hurt you.\n",
      "toxic :  -5.000000074505806 %\n",
      "severe_toxic :  -47.999998927116394 %\n",
      "obscene :  -31.999999284744263 %\n",
      "threat :  -12.999999523162842 %\n",
      "insult :  -40.00000059604645 %\n",
      "identity_hate :  -37.99999952316284 %\n",
      "Similaryty score: 0.699999988079071\n",
      "\n",
      "I will hurt you.\n",
      "toxic :  -5.000000074505806 %\n",
      "severe_toxic :  -47.999998927116394 %\n",
      "obscene :  -31.999999284744263 %\n",
      "threat :  -12.999999523162842 %\n",
      "insult :  -40.00000059604645 %\n",
      "identity_hate :  -37.99999952316284 %\n",
      "Similaryty score: 0.699999988079071\n",
      "\n",
      "I will suffer for you.\n",
      "toxic :  -5.999999865889549 %\n",
      "severe_toxic :  -49.000000953674316 %\n",
      "obscene :  -41.999998688697815 %\n",
      "threat :  -1.9999999552965164 %\n",
      "insult :  -28.999999165534973 %\n",
      "identity_hate :  11.999999731779099 %\n",
      "Similaryty score: 2.4000000953674316\n",
      "\n",
      "I will suffer you.\n",
      "toxic :  -0.9999999776482582 %\n",
      "severe_toxic :  -41.999998688697815 %\n",
      "obscene :  -27.000001072883606 %\n",
      "threat :  -2.9999999329447746 %\n",
      "insult :  -5.999999865889549 %\n",
      "identity_hate :  -7.999999821186066 %\n",
      "Similaryty score: 2.1700000762939453\n",
      "\n"
     ]
    }
   ],
   "source": [
    "make_sugestion(\"I will hurt you\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb3d0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original sentence: You are such an idiot\n",
      "toxic score: 99.00000095367432%\n",
      "severe_toxic score: 3.999999910593033%\n",
      "obscene score: 75.0%\n",
      "threat score: 0.0%\n",
      "insult score: 95.99999785423279%\n",
      "identity_hate score: 0.9999999776482582%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "make_sugestion(\"You are such an idiot\", 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac74b759",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
